{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking with RAGatouille\n",
    "Inspired by : https://github.com/AnswerDotAI/RAGatouille/blob/main/examples/04-reranking.ipynb\n",
    "In this quick example, we'll use the `RAGPretrainedModel` magic class to demonstrate how to **re-rank documents** retrieved by another retriever, such as **your existing RAG pipeline**.\n",
    "First, as usual, let's load up a pre-trained ColBERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "# Loading a pre-trained model for reranking\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is loaded, we must build an index of our documents for our first retrieval step! In the real world, you'd likely already have some sort of pipeline doing this, which we're going to emulate here, using bge embeddings Spotify's excellent `voyager` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from voyager import Index, Space\n",
    "\n",
    "\n",
    "class MyExistingRetrievalPipeline:\n",
    "    index: Index\n",
    "    embedder: SentenceTransformer\n",
    "\n",
    "    def __init__(self, embedder_name: str = \"BAAI/bge-small-en-v1.5\"):\n",
    "        # Initialize the sentence transformer with a specific model\n",
    "        self.embedder = SentenceTransformer(embedder_name)\n",
    "        # Dictionary to store document contents with their index\n",
    "        self.collection_map = {}\n",
    "        # Create vector index using cosine similarity\n",
    "        # num_dimensions matches the embedding size from the model\n",
    "        self.index = Index(\n",
    "            Space.Cosine,\n",
    "            num_dimensions=self.embedder.get_sentence_embedding_dimension(),\n",
    "        )\n",
    "\n",
    "    def index_documents(self, documents: list[str]) -> None:\n",
    "        # Process each document in the input list\n",
    "        for document in documents:\n",
    "            # 1. Convert document content to embedding vector\n",
    "            # 2. Add embedding to index and get back an index ID\n",
    "            # 3. Store original content in collection_map with index ID as key\n",
    "            self.collection_map[\n",
    "                self.index.add_item(self.embedder.encode(document[\"content\"]))\n",
    "            ] = document[\"content\"]\n",
    "\n",
    "    def query(self, query: str, k: int = 10) -> list[str]:\n",
    "        # Convert query text to embedding vector\n",
    "        query_embedding = self.embedder.encode(query)\n",
    "        to_return = []\n",
    "        # Search index for k most similar vectors\n",
    "        # Get their IDs and look up original content\n",
    "        for idx in self.index.query(query_embedding, k=k)[0]:\n",
    "            to_return.append(self.collection_map[idx])\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_pipeline = MyExistingRetrievalPipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our mock of existing pipeline is set up, let's index some documents with it! We'll re-use our favourite combo from the previous examples: `CorpusProcessor` and `get_wikipedia_page()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille.data import CorpusProcessor\n",
    "\n",
    "# Create an instance of CorpusProcessor\n",
    "# This will be used to split documents into smaller, manageable chunks\n",
    "corpus_processor = CorpusProcessor()\n",
    "\n",
    "# Open and read the content of 'text_for_reranking.txt' file\n",
    "with open(\"text_for_reranking.txt\", \"r\") as file:\n",
    "    # Read the entire content of the file into the 'content' variable\n",
    "    content = file.read()\n",
    "\n",
    "# Convert the content into a list containing a single string\n",
    "# This is done because process_corpus expects a list of documents\n",
    "document = [content]\n",
    "\n",
    "# Process the document using CorpusProcessor\n",
    "# chunk_size=100 means the text will be split into chunks of approximately 100 tokens\n",
    "# This helps in:\n",
    "# 1. Making the text more manageable for processing\n",
    "# 2. Creating smaller segments that are better for retrieval\n",
    "# 3. Ensuring chunks aren't too large for the model to handle effectively\n",
    "document = corpus_processor.process_corpus(document, chunk_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add those to the voyager index so we can simulate a real query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_pipeline.index_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does climate change affect agriculture, and what are the impacts on crop yields, farming practices, and food security?\"\n",
    "raw_results = existing_pipeline.query(query, k=7)\n",
    "raw_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh! We can see in the results that we're looking for is explained very clearly:\n",
    "\n",
    ">   Soil quality is also impacted by these changes, as increased heat and water stress can reduce soil fertility and increase erosion. In the long term, these changes could lead to reduced crop yields, particularly in regions already facing food security challenges.\\nClimate Change and Crop Yields\\nRising temperatures are expected to significantly decrease crop yields, especially for staple crops like wheat, rice, and corn. Heat stress during critical stages of crop development can result in lower yields and poorer crop quality. **3th** most relevant result! In a real RAG pipeline, this'd often be well outside the context you'd give to your LLM.\n",
    "\n",
    "This is where ColBERT re-ranking comes into play. Let's use our previously loaded `RAGPretrainedModel` to re-rank the results of our existing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG.rerank(query=query, documents=raw_results, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here it is! The relevant extract is now all the way at the top of the results, ready to be passed to the rest of your pipeline!\n",
    "\n",
    "So why not just use rerank() on the whole index if it's so good? Well, you could, but it's not very efficient. ColBERT is an extremely fast querier, but it needs to have an index built to do so. When you're using ColBERT to rerank documents, it's doing it index-free, which means it needs to encode all your documents and queries, and perform the comparison on the fly. This is fine for a handful of document on CPU or a few hundreds on GPU, but it's going to take exponentially longer as you add more documents!\n",
    "\n",
    "Re-ranking the results of another retrieval method is a good compromise: it allows you to leverage ColBERT's power without having to modify the rest of your pipeline, just increase the `k` value of your retriever and let ColBERT rescore them!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
