{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring your RAG applications using RAGAS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we'll plug in retrieval and augmented generation strategies to our RAG app and evaluate these different strategies using [RAGAS](https://docs.ragas.io/en/stable/concepts/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure you've followed the setup directions in the README. Then, install LlamaIndex, which we'll use to build our RAG workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your knowledge base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the raw Markdown documents and convert them to LlamaIndex nodes, which represent chunks of our source Markdown documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "folder_path = \"mdfiles\"  # Replace with the actual path if different\n",
    "\n",
    "raw_doc_texts = []\n",
    "\n",
    "# Iterate through all files in the specified folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".md\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            markdown_content = file.read()\n",
    "        raw_doc_texts.append(\n",
    "            Document(text=markdown_content, metadata={\"filename\": filename})\n",
    "        )\n",
    "\n",
    "parser = MarkdownNodeParser()\n",
    "base_nodes = parser.get_nodes_from_documents(raw_doc_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a baseline retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, initialize a baseline retriever that fetches the top-k raw text nodes based on embedding similarity to an input query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "\n",
    "model = BedrockEmbedding(\n",
    "    model_name=\"cohere.embed-multilingual-v3\",\n",
    "    credentials_profile_name=\"myprofile\",\n",
    "    region_name=\"eu-west-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "model.get_text_embedding(\"hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 2\n",
    "base_index = VectorStoreIndex(base_nodes, embed_model=model)\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a response synthesizer to help generate the answer to a question based on retrieved context documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.llms.bedrock.base import Bedrock\n",
    "\n",
    "llm = Bedrock(\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    profile_name=\"myprofile\",\n",
    "    region_name=\"eu-west-3\",\n",
    "    temperature=0,\n",
    "    max_tokens=3000,\n",
    ")\n",
    "\n",
    "response_synthesizer_compact = get_response_synthesizer(\n",
    "    response_mode=ResponseMode.COMPACT, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a `retrieve_nodes` function that uses the `base_retriever` to fetch the most relevant context documents given a query. We'll also define an `ask_docs` workflow that combines the retrieval and augmented generation step to return a final answer for a given query.\n",
    "\n",
    "To instrument the retrieval step, we\n",
    "\n",
    "- Decorate the `retrieve_nodes` step with the `retrieval` decorator.\n",
    "- Annotate the span's `input_data` as the input query.\n",
    "- Annotate the span's `output_data` as a list of dictionaries which each represent a single chunk.\n",
    "- Annotate the span's `metadata` with our `top_k` setting.\n",
    "- Tag our retrieval step with the retriever we are using\n",
    "\n",
    "Note that we also return the result of `LLMObs.export_span()` at the end of the `ask_docs` function. We'll need the exported span for later when we submit evaluation results to Datadog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_nodes(query, retriever=base_retriever):\n",
    "    nodes = retriever.retrieve(query)\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def ask_docs(\n",
    "    query, retriever=base_retriever, response_synthesizer=response_synthesizer_compact\n",
    "):\n",
    "    nodes = retrieve_nodes(query, retriever=retriever)\n",
    "    response = response_synthesizer.synthesize(query, nodes=nodes)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RAG workflow is ready! Try a question about LLM Observability. What do you think of the answer quality?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTER_QUESTION = \"What AWS Bedrock and what are its features?\"\n",
    "\n",
    "answer = ask_docs(STARTER_QUESTION, retriever=base_retriever)\n",
    "\n",
    "print(\"Answer: {}\".format(answer))\n",
    "print(\"Context: {}\".format([reference.text for reference in answer.source_nodes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement a recursive retriever and plug that into our `ask_docs` workflow.\n",
    "\n",
    "A recursive retriever first builds a graph of small chunks that have references to larger parent chunks. At query-time, smaller chunks are retrieved first, and then we follow references to bigger chunks. This enhances the context we pass to the augmented generation step. For more information on recursive retrieval, see LlamaIndex's [recursive retrieval](https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes/) guide.\n",
    "\n",
    "Since our raw documents are in Markdown, there's already an implicit parent-child relationship between different text chunks. LlamaIndex provides helpful utility functions to automatically parse these relationships and form an index that is searchable using their `RecursiveRetriever` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "\n",
    "\n",
    "sub_chunk_sizes = [256, 512]\n",
    "sub_node_parsers = [\n",
    "    SentenceSplitter(chunk_size=c, chunk_overlap=20) for c in sub_chunk_sizes\n",
    "]\n",
    "\n",
    "all_nodes = []\n",
    "for base_node in tqdm(base_nodes):\n",
    "    for n in sub_node_parsers:\n",
    "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # also add original node to node\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)\n",
    "\n",
    "print(\"Nodes Created\")\n",
    "all_nodes_dict = {n.node_id: n for n in all_nodes}\n",
    "\n",
    "print(\"Creating Vector Store\")\n",
    "vector_index_chunk = VectorStoreIndex(all_nodes, embed_model=model)\n",
    "\n",
    "\n",
    "print(\"Creating Retriever\")\n",
    "vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=TOP_K)\n",
    "\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the answer improved from our earlier step...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = ask_docs(STARTER_QUESTION, retriever=recursive_retriever)\n",
    "\n",
    "print(\"Answer: {}\".format(answer))\n",
    "print(\"Context: {}\".format([reference.text for reference in answer.source_nodes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the context differ for our two different retrieval strategies, and ultimately, which response do you think is better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS Setup\n",
    "\n",
    "Suppose you wanted to deploy both the baseline retriever and recursive retriever and evaluate how well each retrieval strategy is doing in a production environment. Our LLM Observability SDK enables this through the `submit_evaluation` function.\n",
    "\n",
    "As an example, we'll use the RAGAS open source library to evaluate our RAG workflow. It's powered by LLM-assisted evaluations that measure the performance of your retrievals, augmented generation, and RAG workflow end-to-end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define a list of questions we'll ask our RAG app. Some RAGAS evaluations also require ground truth answers in relation to a target question, so we'll have to define those as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    \"How do I get started?\",\n",
    "    \"I have a complex chatbot, what root span should I use to represent this bot?\",\n",
    "    \"I have a summarization LLM service with some simple pre-and-post processing steps, what root span should I use to represent this bot?\",\n",
    "    \"I don't want to manually instrument my app. Can I still use LLM Observability?\",\n",
    "    \"What's the ml app tag?\",\n",
    "    \"How can I enable user session tracking?\",\n",
    "]\n",
    "\n",
    "eval_ground_truths = [\n",
    "    \"To get started with LLM Observability, you can build a simple example with the Quickstart, or follow the guide for instrumenting your LLM application. Make sure to grab your Datadog API Key\",\n",
    "    \"You should use an agent root span to represent your complex chatbot.\",\n",
    "    \"You should use a workflow root span to represent your complex chatbot.\",\n",
    "    \"LLM Observability has supported integrations for openai, bedrock, and langchain and these libraries will automatically be traced\",\n",
    "    \"The name of your LLM application, service, or project, under which all traces and spans are grouped. This helps distinguish between different applications or experiments.\",\n",
    "    \"When starting a root span for a new trace or span in a new process, specify the session_id argument with the string ID of the underlying user session. You can also set the session_id field when submitting spans via API.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import RAGAS metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    "    LLMContextPrecisionWithReference,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to enrich each of the RAGAS metrics with some metadata that will be relevant when we submit results to Datadog.\n",
    "\n",
    "We'll split out RAGAS metrics into two categories - `production` and `dev`.\n",
    "\n",
    "`production` evaluations don't require ground truths to compute the final score, meaning they can be continously run against production data, while `dev` evaluations require a ground truth.\n",
    "\n",
    "We also specify that the metric type is type `score`, which tells Datadog the evaluation metric has the value of a continuous float.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Callable\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.metrics import (\n",
    "    LLMContextPrecisionWithReference,\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    ")\n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(\n",
    "    profile_name=\"myprofile\",\n",
    "    region_name=\"eu-west-3\",\n",
    ")\n",
    "\n",
    "bedrock_llm = ChatBedrockConverse(\n",
    "    client=session.client(\"bedrock-runtime\"),\n",
    "    region_name=\"eu-west-3\",\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    base_url=\"https://bedrock-runtime.eu-west-3.amazonaws.com\",\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    client=session.client(\"bedrock-runtime\"),\n",
    "    model_id=\"cohere.embed-multilingual-v3\",\n",
    "    region_name=\"eu-west-3\",\n",
    ")\n",
    "\n",
    "\n",
    "bedrock_llm = LangchainLLMWrapper(bedrock_llm)\n",
    "bedrock_embeddings = LangchainEmbeddingsWrapper(bedrock_embeddings)\n",
    "\n",
    "\n",
    "class RagasMetric(TypedDict):\n",
    "    function: Callable\n",
    "    category: str\n",
    "    metric_type: str\n",
    "\n",
    "\n",
    "ragas_metrics = {\n",
    "    Faithfulness.name: RagasMetric(\n",
    "        function=Faithfulness(llm=bedrock_llm), category=\"prod\", metric_type=\"score\"\n",
    "    ),\n",
    "    ResponseRelevancy.name: RagasMetric(\n",
    "        function=ResponseRelevancy(llm=bedrock_llm),\n",
    "        category=\"prod\",\n",
    "        metric_type=\"score\",\n",
    "    ),\n",
    "    LLMContextPrecisionWithReference.name: RagasMetric(\n",
    "        function=LLMContextPrecisionWithReference(llm=bedrock_llm),\n",
    "        category=\"dev\",\n",
    "        metric_type=\"score\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an `EvaluationData` class where we'll save our inference results to later evaluate on. We want to keep track of question, answer, and contexts as inputs to the RAGAS evaluations.\n",
    "\n",
    "We also track\n",
    "\n",
    "1. An exported span so we can tie each evaluation to a specific run of our RAG workflow\n",
    "2. Tags on our evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationData(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    contexts: list[str]\n",
    "    tags: dict[str, str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `run_simulation` function will take a list of evaluation questions and run our RAG app using the specified RAG configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(\n",
    "    questions,\n",
    "    ground_truths,\n",
    "    retrievers=[base_retriever, recursive_retriever],\n",
    "    response_modes=[\"compact\"],\n",
    "):\n",
    "\n",
    "    simulation_results = []\n",
    "\n",
    "    for mode in response_modes:\n",
    "\n",
    "        response_synthesizer = response_synthesizer_compact\n",
    "\n",
    "        for retrieval_strategy in retrievers:\n",
    "\n",
    "            for question, ground_truth in tqdm(\n",
    "                zip(questions, ground_truths), total=len(questions)\n",
    "            ):\n",
    "\n",
    "                answer = ask_docs(\n",
    "                    question,\n",
    "                    retriever=retrieval_strategy,\n",
    "                    response_synthesizer=response_synthesizer,\n",
    "                )\n",
    "\n",
    "                simulation_results.append(\n",
    "                    EvaluationData(\n",
    "                        question=question,\n",
    "                        answer=str(answer),\n",
    "                        ground_truth=ground_truth,\n",
    "                        contexts=[r.text for r in answer.source_nodes],\n",
    "                        tags={\n",
    "                            \"retriever\": (\n",
    "                                \"recursive\"\n",
    "                                if retrieval_strategy == recursive_retriever\n",
    "                                else \"base\"\n",
    "                            ),\n",
    "                            \"response_mode\": mode,\n",
    "                            \"top_k\": TOP_K,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "    return simulation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the evaluation results using both our baseline and recursive retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = run_simulation(\n",
    "    eval_questions, eval_ground_truths, retrievers=[base_retriever, recursive_retriever]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to run RAGAS evaluations and submit the evaluations to Datadog\n",
    "\n",
    "We use the `submit_evaluation` function to send custom evaluation metric data to Datadog.\n",
    "\n",
    "1. Since each evaluation is tied to a span, we used the exported span returned from the earlier function call and pass that into `submit_evaluation`.\n",
    "2. You have to specify the metric type as `score` or `categorical` for each metric you submit to Datadog. So far, all the RAGAS metrics we've used are `score` metrics. However, RAGAS [aspect critiques](https://docs.ragas.io/en/stable/concepts/metrics/critique.html) would be submitted as categorical type evaluation metrics.\n",
    "3. We also tag our evaluation metric with some metadata about the RAG strategy and metric category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def run_ragas(evaluation_data, ragas_metrics):\n",
    "    \"\"\"\n",
    "    Run Ragas evaluation and generate a comprehensive summary DataFrame.\n",
    "\n",
    "    Args:\n",
    "        evaluation_data (list): List of evaluation data dictionaries\n",
    "        ragas_metrics (dict): Dictionary of Ragas metrics to evaluate\n",
    "\n",
    "    Returns:\n",
    "        tuple: A pair of pandas DataFrames (detailed metrics, aggregated metrics)\n",
    "    \"\"\"\n",
    "    # Initialize lists to collect metric results\n",
    "    results_list = []\n",
    "\n",
    "    for span_data in tqdm(evaluation_data, desc=\"Running Ragas Evaluation\"):\n",
    "        # Prepare input dataset\n",
    "        ragas_input = Dataset.from_dict(\n",
    "            {\n",
    "                \"question\": [span_data[\"question\"]],\n",
    "                \"answer\": [span_data[\"answer\"]],\n",
    "                \"contexts\": [span_data[\"contexts\"]],\n",
    "                \"ground_truth\": [span_data.get(\"ground_truth\", \"\")],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Evaluate metrics\n",
    "        results = evaluate(\n",
    "            ragas_input,\n",
    "            [metric[\"function\"] for metric in ragas_metrics.values()],\n",
    "            llm=bedrock_llm,\n",
    "            embeddings=bedrock_embeddings,\n",
    "        )\n",
    "\n",
    "        # Convert results to dictionary\n",
    "        results_df = results.to_pandas().to_dict(\"index\")[0]\n",
    "\n",
    "        # Prepare a dictionary to store metric results\n",
    "        metrics_row = {\n",
    "            \"question\": span_data[\"question\"],\n",
    "            \"retriever\": span_data[\"tags\"][\"retriever\"],\n",
    "            \"response_mode\": span_data[\"tags\"][\"response_mode\"],\n",
    "            \"top_k\": span_data[\"tags\"][\"top_k\"],\n",
    "        }\n",
    "\n",
    "        # Collect non-NaN metrics\n",
    "        for metric_name, metric_config in ragas_metrics.items():\n",
    "            metric_val = results_df[metric_name]\n",
    "            if not math.isnan(metric_val):\n",
    "                metrics_row[metric_name] = metric_val\n",
    "                metrics_row[f\"{metric_name}_category\"] = metric_config[\"category\"]\n",
    "                metrics_row[f\"{metric_name}_type\"] = metric_config[\"metric_type\"]\n",
    "\n",
    "        results_list.append(metrics_row)\n",
    "\n",
    "    # Create a DataFrame from the collected results\n",
    "    summary_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Compute overall summary statistics\n",
    "    aggregation_columns = [\"retriever\", \"response_mode\", \"top_k\"]\n",
    "    metric_columns = [col for col in summary_df.columns if col in ragas_metrics.keys()]\n",
    "\n",
    "    summary_stats = summary_df.groupby(aggregation_columns)[metric_columns].agg(\n",
    "        [\"mean\", \"std\"]\n",
    "    )\n",
    "\n",
    "    return summary_df, summary_stats\n",
    "\n",
    "\n",
    "# Optional: Enhanced display function for better readability\n",
    "def display_ragas_results(summary_df, summary_stats):\n",
    "    \"\"\"\n",
    "    Print Ragas evaluation results in a formatted manner.\n",
    "\n",
    "    Args:\n",
    "        summary_df (pd.DataFrame): Detailed metrics DataFrame\n",
    "        summary_stats (pd.DataFrame): Aggregated metrics DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Detailed Metrics:\")\n",
    "    print(summary_df)\n",
    "    print(\"\\nAggregated Metrics:\")\n",
    "    print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "full_metrics_df, aggregated_metrics_df = run_ragas(evaluation_data, ragas_metrics)\n",
    "display_ragas_results(full_metrics_df, aggregated_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
